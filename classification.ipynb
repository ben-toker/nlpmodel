{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732581648
        }
      },
      "outputs": [],
      "source": [
        "#First things first, we want to import our dataset and load it into a variable we can work with. We also want to load the transformer model we're working with,\n",
        "#which in this case is DistilBERT. \n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"reuters21578\", \"ModApte\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732581773
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#If you visualize the dataset, there's a lot of columns we just do not need. We're going to pare this down to the \"text\" and \"label\" column.\n",
        "from transformers import DistilBertTokenizer\n",
        "from datasets import DatasetDict\n",
        "\n",
        "def has_topic(example):\n",
        "    # Returns True if 'topics' key exists and has at least one topic\n",
        "    return 'topics' in example and bool(example['topics'])\n",
        "\n",
        "dataset = dataset.filter(has_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732581853
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# A lot of entries in the topics column is a list of multiple relevant topics. We're just going to take the first one and call it \"label\" to simplify things.\n",
        "def preprocess(examples):\n",
        "    # examples['topics'] is a list of lists\n",
        "    examples['label'] = [topic[0] if topic else None for topic in examples['topics']]\n",
        "    return examples\n",
        "\n",
        "# Apply the preprocess function to the dataset\n",
        "dataset = dataset.map(preprocess, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732581933
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "columns_to_remove = ['text_type', 'topics', 'lewis_split', 'cgis_split', 'old_id', 'new_id', 'places', 'people', 'orgs', 'exchanges', 'date', 'title']\n",
        "dataset = dataset.remove_columns(columns_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732582003
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Extract all unique labels from the dataset again\n",
        "unique_labels = sorted(set(label for split in dataset.keys() for label in dataset[split]['label']))\n",
        "\n",
        "# Create the label_to_id mapping\n",
        "label_to_id = {label: idx for idx, label in enumerate(sorted(set(dataset[\"train\"][\"label\"])))}\n",
        "\n",
        "#Inverse mapping so we can extract the label from the ID outputted by the model; we unfortunately can't leave it in the dataset.\n",
        "id_to_label = {id: label for label, id in label_to_id.items()}\n",
        "\n",
        "predicted_label_ids = [0, 2, 1]  # Example list of predicted label IDs from your model\n",
        "predicted_labels = [id_to_label[label_id] for label_id in predicted_label_ids]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Saving the label_to_id mapping will make it possible to access predictions from the model seperately after saving!\n",
        "with open('label_to_id.json', 'w') as f:\n",
        "    json.dump(label_to_id, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now predicted_labels will contain the actual labels corresponding to the predicted label IDs\n",
        "print(predicted_labels)\n",
        "\n",
        "# Inspect the label_to_id mapping\n",
        "print(label_to_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732582083
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def label_to_int(example):\n",
        "    # Convert each label in the batch to its corresponding integer ID\n",
        "    example['labels'] = [label_to_id.get(label, -1) for label in example['label']]\n",
        "    return example\n",
        "\n",
        "# Apply the label_to_int function to all splits in the dataset\n",
        "dataset = dataset.map(label_to_int, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732583697
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Now we load DistilBERT and format the dataset in a way that this model can understand (tokenize)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove the 'label' column after encoding the labels\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now the dataset will be tokenized and have our text and numericalized labels\n",
        "import pandas as pd\n",
        "pd.DataFrame(dataset[\"train\"]).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732586577
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "num_labels = len(set(dataset[\"train\"][\"labels\"]))  # This will inform our model of how many unique labels it needs to train on.\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732587853
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory for model checkpoints\n",
        "    num_train_epochs=30,              # number of training epochs\n",
        "    per_device_train_batch_size=16,  # batch size for training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    learning_rate=2e-5,              # learning rate\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    # Options are 'micro', 'macro', 'weighted', or None (gives the scores for each class)\n",
        "    average_type = 'macro'  # or 'macro' or 'weighted' or None\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=average_type)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\n",
        "        'accuracy': acc, # <- mostly interested in this one!\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    compute_metrics=compute_metrics  # Pass the compute_metrics function\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732596996
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "unique_labels = set()\n",
        "for split in dataset.keys():\n",
        "    unique_labels.update(dataset[split]['labels'])\n",
        "print(unique_labels)\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(tokenized_dataset['train']).tail()\n",
        "\n",
        "#because negative numericalized labels break the model hhaaAHHHH\n",
        "def filter_negative_labels(example):\n",
        "    # Returns True if the label is non-negative\n",
        "    return example['labels'] >= 0\n",
        "\n",
        "dataset = dataset.filter(filter_negative_labels)\n",
        "tokenized_dataset = tokenized_dataset.filter(filter_negative_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732597074
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "unique_labels_train = set(tokenized_dataset['train']['labels'])\n",
        "unique_labels_test = set(tokenized_dataset['test']['labels'])\n",
        "print(\"Unique labels in train set:\", unique_labels_train)\n",
        "print(\"Unique labels in test set:\", unique_labels_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732601440
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "unique_labels = set()\n",
        "for split in dataset.keys():\n",
        "    unique_labels.update(dataset[split]['labels'])\n",
        "print(unique_labels)\n",
        "\n",
        "pandas.DataFrame(tokenized_dataset['train']).tail()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732681455
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "trainer.train(resume_from_checkpoint = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_path = \"./Classification_Model/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "trainer.save_model(save_path)\n",
        "model.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1706732683642
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#This is the text we want to test the model on\n",
        "text=\"Bubble teas fall under two categories: teas without milk and milk teas. Both varieties come with a choice of black, green, or oolong tea as the base.[1] Milk teas usually include powdered or fresh milk, but may also use condensed milk, almond milk, soy milk, or coconut milk\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Class ID: 70, Predicted Class Label: tea\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "modelpath = save_path\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelpath)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(modelpath)\n",
        "\n",
        "# Encode the input by tokenizing it so that it matches the dataset\n",
        "encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    logits = model(**encoded_input).logits\n",
        "\n",
        "# Convert logits to probabilities (softmax) and then to the class label\n",
        "probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
        "predicted_class_id = probabilities.argmax().item()\n",
        "\n",
        "# Ensure you have created the id_to_label dictionary as shown before\n",
        "predicted_class_label = id_to_label[predicted_class_id]\n",
        "\n",
        "print(f\"Predicted Class ID: {predicted_class_id}, Predicted Class Label: {predicted_class_label}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
